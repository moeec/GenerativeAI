{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvBHYcpd4v_1"
   },
   "source": [
    "# System setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8pzPvghP5TBs"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOGETHER_API_KEY = os.getenv(\"private\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = 'https://api.together.xyz/inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8IF2Fa6tg_w1"
   },
   "outputs": [],
   "source": [
    "# Decoding parameters\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS = 512\n",
    "TOP_P = 1.0\n",
    "TOP_K = 50\n",
    "REPETITION_PENALTY = 1.0\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gt80J3Zy5rM4"
   },
   "outputs": [],
   "source": [
    "def query_together_endpoint(prompt):\n",
    "    response = requests.post(\n",
    "        ENDPOINT,\n",
    "        json={\n",
    "            \"model\": \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "            \"messages\": prompt,\n",
    "            \"max_tokens\": MAX_TOKENS,\n",
    "            \"temperature\": TEMPERATURE,\n",
    "            \"top_p\": TOP_P,\n",
    "            \"top_k\": TOP_K,\n",
    "            \"repetition_penalty\": REPETITION_PENALTY,\n",
    "            \"stop\": [\"<|eot_id|>\", \"<|eom_id|>\"],\n",
    "            \"stream\": False,\n",
    "        },\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        },\n",
    "    ).json()\n",
    "    return response['output']['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXJZqpiV1_Pp"
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "p-piCAwug9fN"
   },
   "outputs": [],
   "source": [
    "def query_model(user_prompt, system_prompt=None, trigger = None, verbose=True, **kwargs):\n",
    "    # Format the user and system prompts\n",
    "    system_prompt = system_prompt or \"You are a helpful assistant.\"\n",
    "    inst_prompt = f\"{B_INST} {user_prompt} {E_INST}\"\n",
    "    \n",
    "    # Add trigger if provided\n",
    "    if trigger:\n",
    "        inst_prompt = inst_prompt + trigger\n",
    "    \n",
    "    # Prepare the system and user messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    \n",
    "    # Call the Together API with the messages\n",
    "    generation = query_together_endpoint(messages)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"*** System Prompt ***\\n{system_prompt}\")\n",
    "        print(f\"*** User Prompt ***\\n{user_prompt}\")\n",
    "        print(f\"*** Full Messages ***\\n{messages}\")\n",
    "        print(f\"*** Generation ***\\n{generation}\")\n",
    "    \n",
    "    return generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZhFzjfQ2CAg"
   },
   "source": [
    "## System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "h8w88wHjt5X2"
   },
   "outputs": [],
   "source": [
    "ANSWER_STAGE = \"Provide the direct answer to the user question.\"\n",
    "REASONING_STAGE = \"Describe the step by step reasoning to find the answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e2Oxy5RTs20Z"
   },
   "outputs": [],
   "source": [
    "# System prompt can be constructed in two ways:\n",
    "# 1) Answering the question first or\n",
    "# 2) Providing the reasoning first\n",
    "\n",
    "# Similar ablation performed in \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\"\n",
    "# https://arxiv.org/pdf/2201.11903.pdf\n",
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"{b_sys}Answer the user's question using the following format:\n",
    "1) {stage_1}\n",
    "2) {stage_2}{e_sys}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain of thought trigger from \"Large Language Models are Zero-Shot Reasoners\"\n",
    "# https://arxiv.org/abs/2205.11916\n",
    "COT_TRIGGER = \"\\n\\nA: Lets think step by step:\"\n",
    "A_TRIGGER = \"\\n\\nA:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT7pJzdi2M-8"
   },
   "source": [
    "## User prompt for our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "iEUcXYNckT6d"
   },
   "outputs": [],
   "source": [
    "user_prompt_template = \"Q: Llama 2 has a context window of {atten_window} tokens. \\\n",
    "If we are reserving {max_token} of them for the LLM response, \\\n",
    "the system prompt uses {sys_prompt_len}, \\\n",
    "the chain of thought trigger uses only {trigger_len}, \\\n",
    "and finally the conversational history uses {convo_history_len}, \\\n",
    "how many can we use for the user prompt?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "atten_window = 4096\n",
    "max_token = 512\n",
    "sys_prompt_len = 124\n",
    "trigger_len = 11\n",
    "convo_history_len = 390\n",
    "\n",
    "user_prompt = user_prompt_template.format(\n",
    "    atten_window=atten_window,\n",
    "    max_token=max_token,\n",
    "    sys_prompt_len=sys_prompt_len,\n",
    "    trigger_len=trigger_len,\n",
    "    convo_history_len=convo_history_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYozeQNor7fd",
    "outputId": "240f1fc1-fb29-4ec8-abd5-1d233845746d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3059"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_numeric_answer = atten_window - max_token - sys_prompt_len - trigger_len - convo_history_len\n",
    "desired_numeric_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7rs_lWP2VWF"
   },
   "source": [
    "## Testing the prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTOKsW82IIxP",
    "outputId": "2e918314-58d9-40b4-f5f0-f02fe9e00817"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mquery_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36mquery_model\u001b[0;34m(user_prompt, system_prompt, trigger, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt},\n\u001b[1;32m     13\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_prompt},\n\u001b[1;32m     14\u001b[0m ]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Call the Together API with the messages\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m generation \u001b[38;5;241m=\u001b[39m \u001b[43mquery_together_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** System Prompt ***\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msystem_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m, in \u001b[0;36mquery_together_endpoint\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_together_endpoint\u001b[39m(prompt):\n\u001b[1;32m      2\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m      3\u001b[0m         ENDPOINT,\n\u001b[1;32m      4\u001b[0m         json\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m         },\n\u001b[1;32m     19\u001b[0m     )\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'output'"
     ]
    }
   ],
   "source": [
    "r = query_model(user_prompt=user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt + system prompt v1: answering first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmkqUpP7J5Zw",
    "outputId": "681caacf-c691-4764-f7b0-d27e765ab72c"
   },
   "outputs": [],
   "source": [
    "system_prompt = SYSTEM_PROMPT_TEMPLATE.format(\n",
    "    b_sys = B_SYS,\n",
    "    stage_1=ANSWER_STAGE,\n",
    "    stage_2=REASONING_STAGE,\n",
    "    e_sys=E_SYS\n",
    ")\n",
    "\n",
    "r2 = query_model(user_prompt=user_prompt, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt + system prompt v2: reasoning first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfPHZ9v-tnPn",
    "outputId": "bfeac801-a82b-430f-a700-accd443ca775"
   },
   "outputs": [],
   "source": [
    "system_prompt = SYSTEM_PROMPT_TEMPLATE.format(b_sys = B_SYS, stage_1=REASONING_STAGE, stage_2=ANSWER_STAGE, e_sys=E_SYS)\n",
    "\n",
    "r3 = query_model(user_prompt=user_prompt, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3584 - (124 + 11 + 390)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt + cot trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r4 = query_model(user_prompt, trigger=COT_TRIGGER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt + \"A:\" trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r5 = query_model(user_prompt, trigger=A_TRIGGER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOiW36Ll4W/LJq40/BjGEnk",
   "include_colab_link": true,
   "mount_file_id": "1SkBFwV9AhTt8ymXpNk2b-7ehiq-TxEb4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
