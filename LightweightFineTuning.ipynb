{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6dffa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import get_peft_model\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['restaurant_ID', 'restaurant_review_ID', 'michelin_label', 'score_total', 'score_food', 'score_service', 'score_decor', 'fame_reviewer', 'reviewscore_food', 'reviewscore_service', 'reviewscore_ambiance', 'reviewscore_waiting', 'reviewscore_value', 'reviewscore_noise', 'review_text', 'review_length'],\n",
      "        num_rows: 116693\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['restaurant_ID', 'restaurant_review_ID', 'michelin_label', 'score_total', 'score_food', 'score_service', 'score_decor', 'fame_reviewer', 'reviewscore_food', 'reviewscore_service', 'reviewscore_ambiance', 'reviewscore_waiting', 'reviewscore_value', 'reviewscore_noise', 'review_text', 'review_length'],\n",
      "        num_rows: 14587\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['restaurant_ID', 'restaurant_review_ID', 'michelin_label', 'score_total', 'score_food', 'score_service', 'score_decor', 'fame_reviewer', 'reviewscore_food', 'reviewscore_service', 'reviewscore_ambiance', 'reviewscore_waiting', 'reviewscore_value', 'reviewscore_noise', 'review_text', 'review_length'],\n",
      "        num_rows: 14587\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['restaurant_ID', 'restaurant_review_ID', 'michelin_label', 'score_total', 'score_food', 'score_service', 'score_decor', 'fame_reviewer', 'reviewscore_food', 'reviewscore_service', 'reviewscore_ambiance', 'reviewscore_waiting', 'reviewscore_value', 'reviewscore_noise', 'review_text', 'review_length'],\n",
      "    num_rows: 116693\n",
      "})\n",
      "{'restaurant_ID': 258641, 'restaurant_review_ID': '258641_24', 'michelin_label': 0, 'score_total': 5.8, 'score_food': 6.5, 'score_service': 6.0, 'score_decor': 6.2, 'fame_reviewer': 'Fijnproever', 'reviewscore_food': 10, 'reviewscore_service': 8, 'reviewscore_ambiance': 8, 'reviewscore_waiting': 1, 'reviewscore_value': 4, 'reviewscore_noise': 2, 'review_text': 'Ik kan kort zijn, zelden zo lekker gegeten. Was via vakantieveilingen. Keus uit vlees of vis. salade of soep vooraf en appeltaart toe. Samen met mijn man alles geprobeerd en echt alles even lekker. Goede huiswijn ( Zuid afrikaan) Perfect uitzicht. Vlotte en vriendelijk bediening. Prima kok die we ook nog even hebben gesproken. \\\\nEcht voor herhaling vatbaar. En dat zal ook zeker gebeuren', 'review_length': 64}\n",
      "116693\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/training\n",
    "# or load the separate splits if the dataset has train/validation/test splits\n",
    "train_dataset = load_dataset(\"cmotions/NL_restaurant_reviews\", split=\"train\")\n",
    "valid_dataset = load_dataset(\"cmotions/NL_restaurant_reviews\", split=\"validation\")\n",
    "test_dataset  = load_dataset(\"cmotions/NL_restaurant_reviews\", split=\"test\")\n",
    "\n",
    "#look at full dataset\n",
    "dataset = load_dataset(\"cmotions/NL_restaurant_reviews\")\n",
    "print(dataset)\n",
    "\n",
    "#looks at characteristics of train\n",
    "print(train_dataset)\n",
    "\n",
    "# Inspect the first example. See what a restaurant review looks like\n",
    "print(train_dataset[0])\n",
    "\n",
    "#length of dataset\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(\n",
    "        lambda x: tokenizer(x[\"review_text\"], truncation=True), batched=True\n",
    "    )\n",
    "    \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative review\", 1: \"positive review\"},\n",
    "    label2id={\"negative review\": 0, \"positive review\": 1},\n",
    ")\n",
    "\n",
    "# Unfreezes all model layers, making them trainable \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "    \n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7e8a663",
   "metadata": {},
   "source": [
    "###  ⚠️ IMPORTANT ⚠️\n",
    "\n",
    "Due to workspace storage constraints, you should not store the model weights in the same directory but rather use `/tmp` to avoid workspace crashes which are irrecoverable.\n",
    "Ensure you save it in /tmp always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DistilBertForSequenceClassification' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Saving the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/your_model_name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DistilBertForSequenceClassification' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "# Saving the model\n",
    "model.save(\"/tmp/your_model_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
